# ğŸš€ Portable RAG Agent (í¬í„°ë¸” RAG ì—ì´ì „íŠ¸)

> **A Dockerized, Plug & Play Retrieval Augmented Generation System.** > **ë„ì»¤ ê¸°ë°˜ì˜ í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´ RAG ì‹œìŠ¤í…œ (Local & Hybrid Inference).**

![Python](https://img.shields.io/badge/Backend-FastAPI-009688?style=flat-square&logo=fastapi&logoColor=white)
![Docker](https://img.shields.io/badge/Infrastructure-Docker_Compose-2496ED?style=flat-square&logo=docker&logoColor=white)
![VectorDB](https://img.shields.io/badge/VectorDB-Qdrant-b9305c?style=flat-square)
![AI](https://img.shields.io/badge/AI-LangChain_&_Ollama-white?style=flat-square)

---

## ğŸ‡¬ğŸ‡§ English Description

### Project Overview
This project is a **High-Performance Portable RAG Solution** designed to be infrastructure-agnostic. It leverages **Docker** for orchestration and **FastAPI** for the backend logic.

The key innovation lies in its **"Dynamic Host Mounting"** capability: the containerized agent can securely access, analyze, and vectorize files from the host machine (Windows/Mac/Linux) instantly, without requiring manual data migration.

### Key Features
* **ğŸ— Clean Architecture:** Separation of concerns between Ingestion, Vectorization, and Retrieval.
* **ğŸ”Œ Plug & Play:** One command (`docker-compose up`) to start the entire stack.
* **ğŸ§  Intelligent Routing:** Automatically detects file types (PDF, Python Code, YAML, Markdown) and applies specific chunking strategies (e.g., `RecursiveCharacterTextSplitter` for code).
* **âš¡ CPU Optimized:** Uses `FastEmbed` (ONNX) and Quantized LLMs (Ollama) to run efficiently on standard consumer hardware.
* **ğŸ“‚ Dynamic File System:** Real-time exploration of the host file system via a secure API.

### Tech Stack
* **Backend:** Python 3.11, FastAPI, Pydantic.
* **Orchestration:** Docker, LangChain.
* **Database:** Qdrant (Rust-based Vector DB).
* **Inference:** Ollama (Local LLM) / Open Architectures.

---

## ğŸ‡°ğŸ‡· í”„ë¡œì íŠ¸ ì†Œê°œ (Korean Description)

### ê°œìš” (Overview)
ì´ í”„ë¡œì íŠ¸ëŠ” ì¸í”„ë¼ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ” **ê³ ì„±ëŠ¥ í¬í„°ë¸” RAG ì†”ë£¨ì…˜**ì…ë‹ˆë‹¤. **Docker**ë¥¼ í™œìš©í•œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ê³¼ **FastAPI** ê¸°ë°˜ì˜ ë°±ì—”ë“œ ë¡œì§ì„ ê²°í•©í–ˆìŠµë‹ˆë‹¤.

ê°€ì¥ í° íŠ¹ì§•ì€ **"ë™ì  í˜¸ìŠ¤íŠ¸ ë§ˆìš´íŒ…(Dynamic Host Mounting)"** ê¸°ìˆ ì…ë‹ˆë‹¤. ì»¨í…Œì´ë„ˆí™”ëœ ì—ì´ì „íŠ¸ê°€ í˜¸ìŠ¤íŠ¸ ë¨¸ì‹ (Windows/Mac/Linux)ì˜ íŒŒì¼ì— ì•ˆì „í•˜ê²Œ ì ‘ê·¼í•˜ì—¬, ìˆ˜ë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì´ë™í•  í•„ìš” ì—†ì´ ì¦‰ì‹œ ë¶„ì„ ë° ë²¡í„°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥ (Key Features)
* **ğŸ— í´ë¦° ì•„í‚¤í…ì²˜:** ë°ì´í„° ìˆ˜ì§‘(Ingestion), ë²¡í„°í™”(Vectorization), ê²€ìƒ‰(Retrieval) ë¡œì§ì˜ ëª…í™•í•œ ë¶„ë¦¬.
* **ğŸ”Œ í”ŒëŸ¬ê·¸ ì•¤ í”Œë ˆì´:** `docker-compose up` ëª…ë ¹ì–´ í•˜ë‚˜ë¡œ ì „ì²´ ì„œë¹„ìŠ¤ ìŠ¤íƒ ì‹¤í–‰.
* **ğŸ§  ì§€ëŠ¥í˜• ë¼ìš°íŒ…:** íŒŒì¼ ìœ í˜•(PDF, Python Code, YAML, Markdown)ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³ , ì´ì— ë§ëŠ” ìµœì ì˜ ì²­í‚¹(Chunking) ì „ëµì„ ì ìš©í•©ë‹ˆë‹¤.
* **âš¡ CPU ìµœì í™”:** `FastEmbed` (ONNX)ì™€ ì–‘ìí™”ëœ LLM(Ollama)ì„ ì‚¬ìš©í•˜ì—¬ ì¼ë°˜ CPU í™˜ê²½ì—ì„œë„ ê³ ì„±ëŠ¥ì„ ë³´ì¥í•©ë‹ˆë‹¤.
* **ğŸ“‚ ë™ì  íŒŒì¼ ì‹œìŠ¤í…œ:** ë³´ì•ˆ APIë¥¼ í†µí•´ í˜¸ìŠ¤íŠ¸ íŒŒì¼ ì‹œìŠ¤í…œì„ ì‹¤ì‹œê°„ìœ¼ë¡œ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ê¸°ìˆ  ìŠ¤íƒ (Tech Stack)
* **Backend:** Python 3.11, FastAPI, Pydantic.
* **Orchestration:** Docker, LangChain.
* **Database:** Qdrant (Rust ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤).
* **Inference:** Ollama (ë¡œì»¬ LLM) / Open Architectures.

---

## ğŸ›  Getting Started (ì‹œì‘í•˜ê¸°)

### 1. Prerequisites
* Docker & Docker Compose installed.
* Ollama installed locally (for LLM inference).

### 2. Configuration
Create a `.env` file in the root directory:
```bash
# Windows Example
HOST_PATH=C:/Users/YourName

# Mac/Linux Example
HOST_PATH=/Users/yourname